% ****** Start of file apssamp.tex ******
%
%   This file is part of the APS files in the REVTeX 4.1 distribution.
%   Version 4.1r of REVTeX, August 2010
%
%   Copyright (c) 2009, 2010 The American Physical Society.
%
%   See the REVTeX 4 README file for restrictions and more information.
%
% TeX'ing this file requires that you have AMS-LaTeX 2.0 installed
% as well as the rest of the prerequisites for REVTeX 4.1
%
% See the REVTeX 4 README file
% It also requires running BibTeX. The commands are as follows:
%
%  1)  latex apssamp.tex
%  2)  bibtex apssamp
%  3)  latex apssamp.tex
%  4)  latex apssamp.tex
%
\documentclass[%
 reprint,
%superscriptaddress,
%groupedaddress,
%unsortedaddress,
%runinaddress,
%frontmatterverbose, 
%preprint,
%showpacs,preprintnumbers,
nofootinbib,
%nobibnotes,
%bibnotes,
 amsmath,amssymb,
 aps,
%pra,
%prb,
%rmp,
%prstab,
%prstper,
%floatfix,
]{revtex4-1}

\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}


\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

%\usepackage{hyperref}% add hypertext capabilities
%\usepackage[mathlines]{lineno}% Enable numbering of text and display math
%\linenumbers\relax % Commence numbering lines

%\usepackage[showframe,%Uncomment any one of the following lines to test 
%%scale=0.7, marginratio={1:1, 2:3}, ignoreall,% default settings
%%text={7in,10in},centering,
%%margin=1.5in,
%%total={6.5in,8.75in}, top=1.2in, left=0.9in, includefoot,
%%height=10in,a5paper,hmargin={3cm,0.8in},
%]{geometry}

\begin{document}

%\preprint{APS/123-QED}

\title{Improving the Q Learning Algorithm with Rudimentary AI}% Force line breaks with \\
%\thanks{A footnote to the article title}%

\author{Burke Brockelbank}

\date{\today}% It is always \today, today,
             %  but any date may be explicitly specified

\begin{abstract}
Neural networks have effectively been taught to play video games. These neural nets have historically been trained through a method of reinforcement learning called Q learning. One of the major challenges with this method occurs in the early stages of training when the quality function is most inaccurate. Here we show a method for skipping past this initial training phase, decreasing the time to convergence. In this method, the neural net first trains on labelled data generated by a sub-optimal, but theoretically simple, artificial intelligence until it is capable of playing the game to a similar effectiveness. It is then trained with Q learning to refine the performance of the AI. This method is applicable to any Q learning scenario where it is easy to concieve an AI to prepare labelled data.
\end{abstract}

\maketitle

%\tableofcontents

\section{\label{sec:Introduction}Introduction}
The method of training explored in this report is Q learning. Q learning has always had the problem of slow convergence near the beginning of its training, an issue that is inherent its formulation. Q learning is shown to theoretically converge in infinite time with some restrictions on the learning rate in \cite{sutton1998reinforcement} and has proven capable of converging with no assistance in practice, however the amount of computation power and time needed to do this may not always be feasible. Additionally, it is better to avoid the problems with Q learning rather than trust in the theory in case a specific implementation makes it impossible to use.

Another issue with reinforcement learning is that its behavior is highly dependent on how the reward function is specified. Poor choices of reward functions can lead the model to make decisions which are not desired. In \cite{clark_amodei_2017} the authors describe an interesting example where the specific reward function they used lead to a difference between how they wanted their player to act and how their player ended up maximizing reward.

\subsection{\label{sec:QualityFunction}The Quality Function}
 The formulation of DQ begins with specifying the reward of an action. Suppose that there is a player in a state $s$ who takes an action $a$. This action will move the player from the state $s$ to $s'$ and also give the player some immediate reward $r(s,a)$. For example, if the player is a robo-advisor that is trading stocks then the state is the current price of all stocks and the action is either to wait, sell a stock, or buy a stock. The reward may be specified as the amount of value gained in moving from the state $s$ to $s'$. After this the player will take the action $a'$ from the state $s'$, followed by taking the action $a''$ from the consequent state $s''$ and so on. The cumulative reward $R$ will include all of these rewards, but to reflect the uncertainty about future actions, a discount factor $\gamma\in(0,1)$ is introduced.
\begin{equation}
  R =  r_{s,a} + \gamma r_{s',a'} + \gamma^2 r_{s'',a''} + \hdots, \label{eq:CumulativeReward}
\end{equation}
Suppose we had some ideal policy function $\pi^*$ that we could consult at every step to give us the ideal action to maximize total reward given a state. Then we define our quality function as,
\begin{equation}
  Q^*(s,a) = r_{s,a} + \gamma r_{s',\pi^*(s')} + \gamma^2 r_{s'',\pi^*(s'')} + \hdots. \label{eq:Bellman1}
\end{equation}
Thus, $Q(s,a)$ represents the maximal cumulative reward we can get from the state $s$ given that we are going to perform the action $a$ from that state. We can rewrite this as,
\begin{equation}
  Q^*(s,a) = r_{s,a} + \gamma Q^*(s', \pi^*(s')). \label{eq:Bellman2}
\end{equation}
which is known as the Bellman equation. Additionally, $\pi^*$ can then be written in terms of $Q^*$ as,
\begin{equation}
  \pi^*(s) = \argmax_a Q^*(s,a). \label{eq:Policy1}
\end{equation}

The objective of Q learning is to use a neural net to model this quality function. Let the approximation be $Q$. Define a policy function analogously to \eqref{eq:Policy1},
\begin{equation}
  \pi(s) = \argmax_a Q(s,a). \label{eq:Policy2}
\end{equation}

 Then there will be some difference between the two sides of \eqref{eq:Bellman2} called $\delta$.
\begin{equation}
  Q(s,a) = r_{s,a} + \gamma Q(s', \pi(s')) + \delta. \label{eq:delta}
\end{equation}
The loss is then calculated as a function of $\delta$.

In order to train the neural net effectively, some degree of exploration is used. In the case of this report, $\pi$ is replaced with $\pi_\epsilon$ where $\epsilon\in[0,1]$. $\pi_\epsilon$ has a probability $\epsilon$ of returning a random action and a $(1-\epsilon)$ probability of returning $\pi$. 

\subsection{\label{sec:Monkey}Monkey Want Banana}
This report considers the performance of 

\subsection{\label{sec:Challenges}Challenges with Q Learning}
One of the main issues with Q learning is evident in \eqref{eq:delta}. Calculating the loss for performing the action $a$ from the state $s$, it depends on the next state. Hypothetically, $\delta$ could be very small or even zero even if $Q$ does not accurately model $Q^*$ as long as $Q(s,a)$ and $\gamma Q(s', \pi(s'))$ differ from their respective $Q^*$ predictions by the same amount. If $Q$ is inaccurate, like at the beginning of its training, it will converge slowly.


\section{\label{sec:Methods}Methods}

\section{\label{sec:Results}Results}

\section{\label{sec:Discussion}Discussion}

\bibliographystyle{apsrev4-1}
\bibliography{bib}

\end{document}
%
% ****** End of file apssamp.tex ******



























