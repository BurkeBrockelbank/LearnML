TESTING LINEAR MODEL AGAINST AI
An AI was developed to navigate the monkey game. This AI was then used to generate over 1.3 million turns of gameplay. A linear model was trained with this data. Reinforcement learning was attempted after this, but no increase in the accuracy of the quality function was found so this training was reverted.

The models were scored against eachother based on the amount of food they could collect in 30 turns averaged over 1000 random placements. This was repeated five times each.

 Model | Score
-------+-------
Linear |  6.45, 6.923, 6.739, 7.158, 6.906
    AI | 39.341, 38.701, 39.334, 40.7, 39.474

It is clear that the linear model for the monkey brain is insufficiently complex for modelling the artificial intelligence.

PROGRESSION TO A FULL CNN
I added a convolutional layer to the progressional brain. The net architecture is
(4x11x11) & 1
(4x11x11) & 1 Conv2d ReLU
485 Flatten
9 Relu
8 Relu
5

It behaves quite well after training on AIDATA\AIData0.txt for 30 epochs in 3 batches each with randomized order between each epoch. This is in contrast to more rudimentary nets. Clearly the convolutional layer makes the network more capable of learning the AI algorithm.

Built gif of performance in ./img/OneConvLayer showing this. The qualities are clearly wrong, but at least the net is making the right decisions. After reinforcement elarning the qualities improve drastically. I will add in more layers.

Added yet another fully-connected layer
(4x11x11) & 1
(4x11x11) Conv2d ReLU & 1
484 Flatten & 1
25 ReLU & 1
26
9 Relu
8 Relu
5

This seems to be important. The supervised training is going a lot better with this layer
added in! The loss is looking to stagnate around 0.45 whereas without this layer it was getting stuck around 0.7.

The result of supervised training actually wasn't too good because the AI often gets stuck, the monkey often just got stuck itself.

This bad result carried over to the curated training. I will try again with fewer epochs.

Did 10 epochs and the loss stopped at 0.67 like the previous successful test. It still didn't work too well. The performance is much better. Adding in the rest of the convolutional layers.

(4x11x11) & 1
(4x11x11) Conv2d ReLU & 1
(6x9x9) Conv2d ReLU & 1
(4x7x7) Conv2d ReLU & 1
(1x5x5) Conv2d ReLU & 1
25 Flatten & 1
26 Concat
9 Relu
8 Relu
5

Supervised training started with higher than expected loss (2.95). Each epoch is now taking less time, but the training rate has decreased and stagnated at a loss of 1.2. Monkey behavior seems fairly random. The monkey is not making intelligent choices. This is not a surprise because the amount of training data was not increased. We will add in another two data files.

Using AIDATA\AIData0.txt, AIDATA\AIData1.txt, AIDATA\AIData2.txt for training. Supervised training seemed totally ineffective. The monkey just sits still. Not staying still anymore. I am going to increase the exploration rate to a constant 0.8 and go for another round of curated reinforcement learning.

The net seems to be insensitive to training.

BRAIN V3: SMALL SCALE UNLEARNABILITY
One of the reasons that the net was unable to learn was that it had too many convolutional layers, leading to a receptive field that is far larger than what we want. The monkey needs to be able to see its near surroundings exactly. As such, a separate branch will be created for this.

Convolutions are specified as (kernel size, padding, stride)

vision                  & vision                        & food
(4x11x11)               & (4x11x11)                     & 1
(4x5x5) Crop            & (8x11x11) Conv2d(3,1,1) ReLU  & 1
(4x5x5)                 & (4x8x8) Conv2d(4,0,1) ReLU    & 1
(4x5x5)                 & (2x5x5) Conv2d(4,0,1) ReLU    & 1
(4x5x5)                 & (2x5x5)                       & 1
150 Flatten             & 1
151 Concat
25 Relu
9 Relu
8 Relu
5


Supervised training on AIData0.txt lr0.001 epoch10 batch3 OptimizerAdam
Curated Reinforcement training level0 lr0.001 gamma0.8 food20 epsilon(0.9,0.1,1000) N4000 OptimizerAdagrad
This architecture seems to be onto something. After supervised training, there was clearly some sort of benefit, although the qualities were inaccurate. When this was turned over to CR learning, the results initially seem phenomenal See V3.png. The monkey is also consistently moving towards bananas.

Trying now this training method:
Supervised training on AIData0.txt lr0.001 epoch10 batch3 max_discount0.05 OptimizerAdam
CR training level0 lr0.001 gamma0.8 food20 epsilon(0.7,0.1,2000) N4000 OptimizerAdam
CR training level1 lr0.001 gamma0.8 food20 epsilon(0.3,0.1,2000) N4000 OptimizerAdam
CR training level0 lr0.001 gamma0.8 food20 epsilon(0.3,0.1,2000) N4000 OptimizerAdam
CR training level1 lr0.001 gamma0.8 food20 epsilon(0.3,0.1,2000) N4000 OptimizerAdam
CR training level2 lr0.001 gamma0.8 food20 epsilon(0.3,0.1,2000) N4000 OptimizerAdam
CR training level1 lr0.001 gamma0.8 food20 epsilon(0.3,0.1,2000) N4000 OptimizerAdam

Overall I am fairly happy with this. I will try now to set it free in the banana room (6% bananas)
T6: DQN lr0.01 gamma0.8 epsilon(0.3,0.1,5000) N50000 OptAdam
T7: CR level1 lr0.001 gamma0.8 food20 epsilon(0.15,0.15,1) N2000 OptAdam
T8: DQN lr0.001 gamma0.8 epsilon(0.3,0.1,5000) N50000 OptAdam

I notice that the monkey moves preferentially to the left. While this is garnering it a decent amount of food, this is not how a human would play, and it is not the most efficient method for moving around. My solution is to do curated training a further distance out so it will learn that it is beneficial to go up and down and not ignore up and down in favour of going left.
T9: CR level3 lr0.001 gamma0.8 food20 epsilon(0.15,0.15,1) N2000 OptAdam

Testing monkeys in random room (6% bananas)
 Model | Score
-------+-------
T9Model|  9.816, 9.828, 9.594, 9.990, 9.680
    AI | 39.407, 38.685, 39.991, 39.789, 39.555

Clearly the model is still insufficiently replicating the AI, however we are seeing an improvement over the linear AI. One of the issues we have is that the monkey is still missing things that are more than a few blocks away in the examples up to now. I think that this is primarily because outside of two blocks away, the monkey doesn't see exactly what is going on around it. This cropping occurs in the brain architecture. I think it is time to allow it to train in the banana room with some more complexity. I will go with a room that includes lava and barriers now. (2% barriers, 6% bananas, 1% lava). However, first I want to check it with curation of level 2 again briefly.


I had a great idea! I made it so that the initial move in curated training is always random. This way it explores in the first move (where it knows nothing) and then rely on its past curation training. I am starting a new training branch, called branch 1. Filenames for brain saves will be of the form B1Tnn.brainsave.

B1T0: CR lvl0 lr0.001 gamma0.8 food20 eps(0.3,0.1,2000) N10000 OptAdam
The monkey is not too accurate. I am not really sure why.
B1T1: CR lvl1 lr0.0005 gamma0.8 food20 eps(0.3,0.1,2000) N10000 OptAdam
I did some experimenting and it is a learning rate problem! The level 1 training does so much better with a smaller learning rate.

B1T0: CR lvl0 lr0.001 gamma0.8 food20 eps(0.3,0.1,2000) N10000 OptAdam rand_start
B1T1: CR lvl0 lr0.001 gamma0.8 food20 eps(0.3,0.1,2000) N3000 OptAdam
B1T2: CR lvl1 lr0.001 gamma0.8 food20 eps(0.3,0.1,2000) N5000 OptAdam

It's not doing too well honestly. I am going to let it do supervised training again. Keeping gamma at 0.8.

B2T0: Supervised AIData0.txt AIData1.txt AIData2.txt lr0.01 epoch10 batch3 max_discount0.05 OptAdagrad

From watching the monkey train, it is clear that the qualities are highly overestimated. This is to be expected because of the sheer number of bananas the AI monkey collects. The monkey seems to make the correct decision for movement some of the time.

B2T1: CR lvl0 lr0.001 gamma0.8 food20 eps(0.3,0.1,2000) N5000 OptAdam

The monkey is making all the right decisions at the 0th level of curation. Clearly pretraining with the AI is extremely important.

B2T2: CR lvl1 lr0.001 gamma0.8 food20 eps(0.3,0.1,2000) N5000 OptAdam

I noticed that before training the monkey was actually making decent decisions. I suppose that this must be due to B2T0 training. After training I don't see any marked improvement honestly. I notice that oftentimes the monkey doesn't know what to do when it gets adjacent to a banana as well. I will just train more and overwrite this last training.

B2T2: CR lvl1 lr0.001 gamma0.8 food20 eps(0.3,0.1,2000) N15000 OptAdam

It really didn't help.

B2T2: CR lvl1 lr0.001 gamma0.8 food20 eps(0.3,0.1,2000) N5000 OptAdam

Performance is quite good.

B2T3: CR lvl0 lr0.001 gamma0.8 food20 eps(0.3,0.1,2000) N5000 OptAdam

The model is a little too optimistic still about the rewards. I am going to let it run for a while in the random room to change that.

B2T4: RR lr0.001 gamma0.8 food20 eps(0.5,0.1,5000) N15000 OptAdam

Already before training I am testing the monkey on 2nd level curationn and it doesn't look bad. After training, the monkey retains its good curated training behavior

Monkey test

Testing monkeys in random room (2% barriers, 6% bananas, 1% lava)
 Model | Score
-------+-------
  B2T4 | 20.487, 21.365, 21.366, 21.068, 20.536
    AI | 31.863, 29.772, 29.655, 30.651, 30.145

This is indeed exciting! As we would expect, the AI does worse when we begin adding barriers (which must be traversed around) and lava. The B2T4 model also, on average, ends its turn with more food than it initally had! this means that, on average, it is self sustaining. Due to fluctuations of course it will still die most likely, but we are getting close to a fully self-sustaining monkey.

With this in mind I will do some level 0 training on lava. Followed by some level 2 training on bananas.

B2T5: CR lvl0 lr0.001 gamma0.8 food20 eps(0.3,0.1,5000) N5000 OptAdam LAVA

Now let's do some level 2 training with bananas.

B2T6: CR lvl2 lr0.001 gamma0.8 food20 eps(0.3,0.1,2000) N10000 OptAdam

The level 2 training didn't go well. The monkey lost its ability to make smart decision. We will revert back to B2T4.

B2T5: RR lr0.001 gamma0.8 food20 eps(0.5,0.05,5000) N50000 OptAdam

Let's test it again.

 Model | Score
-------+-------
  B2T4 | 31.411, 32.226, 31.097, 31.857, 31.087
    AI | 31.863, 29.772, 29.655, 30.651, 30.145

The monkey is about the same as the AI even a little better!!! I will make a gif of it playing (B2T5.gif).

What I would like to do at some point to make it play more like a human is to add some recurrency. However, right now I will just allow it to play for a while to get better. I am also going to reduce the banana frequency to 5%.

B2T6: RR lr0.001 gamma0.8 food20 eps(0.2,0.05,5000) N50000 OptAdam

 Model | Score
-------+-------
  B2T4 | 19.117, 19.650, 19.352, 19.283, 19.721
    AI | 26.500, 25.778, 24.676, 25.086, 23.639

The AI is outperforming again with sparser bananas. I will increase the barrier frequency to 3% and see what happens (#3%, m0%, b5%, d1).

 Model | Score
-------+-------
  B2T4 | 18.901, 17.631, 19.069, 18.12, 17.987

B2T7: RR lr0.001 gamma0.8 food20 eps(0.1,0.05,5000) N50000 OptAdam

 Model | Score
-------+-------
  B2T4 | 26.505, 26.434, 35.357, 24.948, 24.583
    AI | 23.989, 21.699, 22.245, 20.497, 19.907

Watching this monkey go, I wonder if that the entire convolutional branch is pointless. The monkey doesn't seem to be using it at all. Anyway, the monkey is doing better than the AI now that we have added more barriers! That's a good place to stop today.